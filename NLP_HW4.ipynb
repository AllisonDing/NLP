{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNp3CnB/zlf1zGSkfONBQ/u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AllisonDing/NLP/blob/main/NLP_HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE7PnWpdHLn3",
        "outputId": "0e336dfb-b415-47db-9506-fb4a5d07da4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "# Install Gensim if you haven't already\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import download\n",
        "import string\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "z85zX1R3H5UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents\n",
        "documents = [\n",
        "    \"Artificial Intelligence (AI) is a branch of computer science that aims to create systems able to perform tasks that would normally require human intelligence.\",\n",
        "    \"Machine learning is a subset of AI that includes algorithms that give computers the ability to learn from and make predictions on data.\",\n",
        "    \"Deep learning is a subset of machine learning that uses neural networks with many layers, enabling the modeling of complex patterns in data.\",\n",
        "    \"Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns.\",\n",
        "    \"Computer vision is a field of artificial intelligence that trains computers to interpret and understand the visual world.\",\n",
        "    \"Natural language processing (NLP) enables computers to understand and process human languages, facilitating user interactions.\",\n",
        "    \"Data mining is the process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.\",\n",
        "    \"Predictive analytics uses statistical algorithms and machine learning techniques to identify the likelihood of future outcomes based on historical data.\",\n",
        "    \"Recurrent Neural Networks (RNNs) are a type of neural network that are well-suited to processing sequences of data for tasks like speech recognition.\",\n",
        "    \"Generative Adversarial Networks (GANs) consist of two neural networks contesting with each other in a game, typically used in unsupervised learning tasks.\",\n",
        "    \"Reinforcement learning is concerned with how software agents ought to take actions in an environment to maximize some notion of cumulative reward.\",\n",
        "    \"Decision trees are a type of supervised learning algorithm that is used for classification and regression tasks.\",\n",
        "    \"Random forests are an ensemble learning method that operates by constructing a multitude of decision trees at training time to output the class that is the mode of the classes of the individual trees.\",\n",
        "    \"Support vector machines (SVMs) are supervised learning models with associated learning algorithms that analyze data for classification and regression.\",\n",
        "    \"K-means clustering is a type of unsupervised learning, which is used when you have unlabeled data and the goal is to find groups in the data.\",\n",
        "    \"The Principal Component Analysis (PCA) is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\",\n",
        "    \"Gradient boosting is a machine learning technique for regression and classification problems that builds a model from a set of weak prediction models, typically decision trees.\",\n",
        "    \"Anomaly detection is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.\",\n",
        "    \"Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data.\",\n",
        "    \"Text mining, also referred to as text data mining, roughly equivalent to text analytics, is the process of deriving high-quality information from text.\",\n",
        "    \"Bagging, or Bootstrap Aggregating, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms.\",\n",
        "    \"A/B testing, also known as split testing, is a marketing experiment wherein you split your audience to test a number of variations of a campaign and determine which performs better.\",\n",
        "    \"Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases.\",\n",
        "    \"The field of robotics is closely related to AI. Robotics involves building robots that can interact with the physical world.\",\n",
        "    \"Bioinformatics involves the application of computational methods to understand biological data, such as genetic sequences.\",\n",
        "    \"Quantum computing is an emerging technology that uses the principles of quantum mechanics to perform computations more efficiently than traditional computers.\",\n",
        "    \"The Internet of Things (IoT) is a network of physical objects that are embedded with sensors, software, and other technologies for the purpose of connecting and exchanging data with other devices and systems over the internet.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "NBaiUut0JD-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "\n",
        "#load 'stopwords' package that contains lists of stop words, which are common words like \"the,\" \"is,\" \"in,\" etc., that are often removed during text preprocessing to focus on more meaningful words.\n",
        "download('stopwords')\n",
        "#load 'punkt' package that is used for tokenizing text into sentences and words. It contains models for punctuation that helps in splitting text into a list of words or sentences.\n",
        "download('punkt')\n",
        "#load 'wordnet' package that allows access to the WordNet database, which is used for lemmatization. WordNet is a large lexical database of English where nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms (synsets).\n",
        "download('wordnet')\n",
        "#create a set of English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "#lemmatizer is used to reduce words to their base or dictionary form (lemmas)\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv5fcERZJM81",
        "outputId": "3b270b19-074d-4b8f-e73b-af74b5d8d97f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Xot1ldwWqPUg",
        "outputId": "c222f5ae-1c17-4265-9d0b-228c13447278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # create a new tokens list that converts each word into lower case and split the string into individual words or tokens (including punctuations and special characters).\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # create a new tokens list that includes only tokens not in 'stop_words' list and not found in 'string.punctuation' (!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~)\n",
        "    tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "    # create a new tokens list that reduces a word to its base or root form (lemma)\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "OvfC_JyYJgS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new processed_docs list that converts each string in the documents using preprocess_text() function\n",
        "processed_docs = [preprocess_text(doc) for doc in documents]"
      ],
      "metadata": {
        "id": "wMsgre1WJjOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it maps each unique word in the processed documents to a unique integer ID and collects statistics about word freqencies.\n",
        "dictionary = corpora.Dictionary(processed_docs)\n",
        "# each document is transformed into a list of tuples, with each tuple containing a word ID (corresponding to a unique word) and its frequency in that document.\n",
        "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
      ],
      "metadata": {
        "id": "nnRZ58RsLxcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a coherence_scores list that will store coherence score of different model\n",
        "coherence_scores = []\n",
        "# create a num_topics list that will store number of topics parameter\n",
        "num_topics = []\n",
        "\n",
        "# loop over different number of topics\n",
        "for i in range(2, 10):\n",
        "  # Build LDA model\n",
        "  lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=i, passes=50)\n",
        "  # Evaluate the quality of the topics that a topic model (LDA) has learned.\n",
        "  coherence_model = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "  # Compute coherence store of a coherence model\n",
        "  coherence_score = coherence_model.get_coherence()\n",
        "  # Append coherence score of each model to the coherence_scores list\n",
        "  coherence_scores.append(coherence_score)\n",
        "  # Append number of topics parameter to the num_topics list\n",
        "  num_topics.append(i)"
      ],
      "metadata": {
        "id": "JqNHAirve0Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# identify the index for the highest coherence score\n",
        "max_index = np.argmax(coherence_scores)\n",
        "# identify the highest coherence score\n",
        "best_coherence_score = coherence_scores[max_index]\n",
        "# identify the number of topics parameter (model) that has the highest coherence score\n",
        "best_num_topics = num_topics[max_index]\n",
        "print(f\"Number of Topics: {best_num_topics}, Coherence Score: {best_coherence_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkRT0YxXivUM",
        "outputId": "54d59d4f-e98f-40be-f543-c28648a06037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Topics: 4, Coherence Score: 0.48281782896858266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LDA model that has the highest coherence score\n",
        "best_lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=4, passes=50)"
      ],
      "metadata": {
        "id": "MX-cZJ6mkWbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the topics\n",
        "for topic_id, topic in best_lda_model.print_topics():\n",
        "    print(f\"Topic {topic_id}: {topic}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2YGyOyfkdAa",
        "outputId": "85a9f3e3-6b82-4d4a-b639-8f43d54ebe59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 0: 0.037*\"learning\" + 0.032*\"data\" + 0.032*\"machine\" + 0.027*\"set\" + 0.026*\"large\" + 0.021*\"text\" + 0.016*\"method\" + 0.016*\"mining\" + 0.011*\"ai\" + 0.011*\"model\"\n",
            "Topic 1: 0.020*\"human\" + 0.020*\"network\" + 0.020*\"algorithm\" + 0.020*\"internet\" + 0.020*\"language\" + 0.011*\"system\" + 0.011*\"physical\" + 0.011*\"software\" + 0.011*\"computer\" + 0.011*\"designed\"\n",
            "Topic 2: 0.046*\"learning\" + 0.024*\"data\" + 0.024*\"machine\" + 0.024*\"us\" + 0.017*\"tree\" + 0.017*\"algorithm\" + 0.017*\"class\" + 0.017*\"quantum\" + 0.009*\"decision\" + 0.009*\"classification\"\n",
            "Topic 3: 0.034*\"data\" + 0.023*\"network\" + 0.017*\"computer\" + 0.017*\"neural\" + 0.017*\"task\" + 0.017*\"intelligence\" + 0.017*\"learning\" + 0.012*\"used\" + 0.012*\"type\" + 0.012*\"time\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hNvhfSaFkwAS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}